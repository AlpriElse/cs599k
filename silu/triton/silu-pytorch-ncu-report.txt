==PROF== Connected to process 31440 (/usr/bin/python3.10)
==PROF== Profiling "distribution_elementwise_grid..." - 0 (1/10): 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 1 (2/10): 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 2 (3/10): 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 3 (4/10): 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 4 (5/10): 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 5 (6/10): 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 6 (7/10): 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 7 (8/10): 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 8 (9/10): 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 9 (10/10): 0%....50%....100% - 10 passes
Average time: 0.6374535673856735 ms
Elements: 268,435,456
Bytes per element: 4
Total data processed: 2.15 GB
Memory throughput: 3368.85 GB/s
==PROF== Disconnected from process 31440
[31440] python3.10@127.0.0.1
  void unnamed>::distribution_elementwise_grid_stride_kernel<float, 4, void templates::normal_and_transform<float, float, CUDAGeneratorImpl *, void templates::normal_kernel<CUDAGeneratorImpl *>(const TensorBase &, double, double, T1)::[lambda() (instance 1)]::operator ()() lambda() (instance 2)]::operator ()() lambda(float) (instance 1)]>(TensorIteratorBase &, T3, T4)::[lambda(curandStatePhilox4_32_10 *) (instance 2)], void unnamed>::distribution_nullary_kernel<float, float, float4, CUDAGeneratorImpl *, void templates::normal_and_transform<float, float, CUDAGeneratorImpl *, void templates::normal_kernel<CUDAGeneratorImpl *>(const TensorBase &, double, double, T1)::[lambda() (instance 1)]::operator ()() lambda() (instance 2)]::operator ()() lambda(float) (instance 1)]>(TensorIteratorBase &, T3, T4)::[lambda(curandStatePhilox4_32_10 *) (instance 2)], void templates::normal_kernel<CUDAGeneratorImpl *>(const TensorBase &, double, double, T1)::[lambda() (instance 1)]::operator ()() lambda() (instance 2)]::operator ()() lambda(float) (instance 1)]>(TensorIteratorBase &, T4, const T5 &, T6)::[lambda(int, float) (instance 1)]>(long, PhiloxCudaState, T3, T4) (1056, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         2.62
    SM Frequency                    Ghz         1.52
    Elapsed Cycles                cycle      1187722
    Memory Throughput                 %        33.42
    DRAM Throughput                   %        33.42
    Duration                         us       777.95
    L1/TEX Cache Throughput           %        21.60
    L2 Cache Throughput               %        32.16
    SM Active Cycles              cycle   1176713.90
    Compute (SM) Throughput           %        77.14
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1056
    Registers Per Thread             register/thread              42
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Stack Size                                                  1024
    Threads                                   thread          270336
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                1.60
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 397 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 50.0% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            5
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           40
    Theoretical Occupancy                     %        62.50
    Achieved Occupancy                        %        50.40
    Achieved Active Warps Per SM           warp        32.26
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 19.35%                                                                                    
          The difference between calculated theoretical (62.5%) and measured achieved occupancy (50.4%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 37.5%                                                                                     
          The 10.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the      
          hardware maximum of 16. This kernel's theoretical occupancy (62.5%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    680967.33
    Total DRAM Elapsed Cycles        cycle     97793536
    Average L1 Active Cycles         cycle   1176713.90
    Total L1 Elapsed Cycles          cycle    156425494
    Average L2 Active Cycles         cycle   1317483.83
    Total L2 Elapsed Cycles          cycle    127363776
    Average SM Active Cycles         cycle   1176713.90
    Total SM Elapsed Cycles          cycle    156425494
    Average SMSP Active Cycles       cycle   1176684.47
    Total SMSP Elapsed Cycles        cycle    625701976
    -------------------------- ----------- ------------

  void vectorized_elementwise_kernel<4, unnamed>::silu_kernel(TensorIteratorBase &)::[lambda() (instance 1)]::operator ()() lambda() (instance 2)]::operator ()() lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3) (524288, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         2.62
    SM Frequency                    Ghz         1.44
    Elapsed Cycles                cycle       849209
    Memory Throughput                 %        90.62
    DRAM Throughput                   %        90.62
    Duration                         us       582.37
    L1/TEX Cache Throughput           %        33.79
    L2 Cache Throughput               %        82.98
    SM Active Cycles              cycle    846423.42
    Compute (SM) Throughput           %        48.72
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 524288
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Stack Size                                                  1024
    Threads                                   thread        67108864
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                              248.24
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block           21
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        83.97
    Achieved Active Warps Per SM           warp        53.74
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 16.03%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (84.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1382092
    Total DRAM Elapsed Cycles        cycle     73205760
    Average L1 Active Cycles         cycle    846423.42
    Total L1 Elapsed Cycles          cycle    110844140
    Average L2 Active Cycles         cycle    986131.69
    Total L2 Elapsed Cycles          cycle     95211072
    Average SM Active Cycles         cycle    846423.42
    Total SM Elapsed Cycles          cycle    110844140
    Average SMSP Active Cycles       cycle    849032.74
    Total SMSP Elapsed Cycles        cycle    443376560
    -------------------------- ----------- ------------

  void vectorized_elementwise_kernel<4, unnamed>::silu_kernel(TensorIteratorBase &)::[lambda() (instance 1)]::operator ()() lambda() (instance 2)]::operator ()() lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3) (524288, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         2.62
    SM Frequency                    Ghz         1.48
    Elapsed Cycles                cycle       866591
    Memory Throughput                 %        90.67
    DRAM Throughput                   %        90.67
    Duration                         us       582.05
    L1/TEX Cache Throughput           %        33.46
    L2 Cache Throughput               %        83.04
    SM Active Cycles              cycle    854771.02
    Compute (SM) Throughput           %        47.64
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 524288
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Stack Size                                                  1024
    Threads                                   thread        67108864
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                              248.24
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block           21
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        83.92
    Achieved Active Warps Per SM           warp        53.71
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 16.08%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (83.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   1382175.50
    Total DRAM Elapsed Cycles        cycle     73168000
    Average L1 Active Cycles         cycle    854771.02
    Total L1 Elapsed Cycles          cycle    113358744
    Average L2 Active Cycles         cycle    985633.82
    Total L2 Elapsed Cycles          cycle     95156928
    Average SM Active Cycles         cycle    854771.02
    Total SM Elapsed Cycles          cycle    113358744
    Average SMSP Active Cycles       cycle    848919.84
    Total SMSP Elapsed Cycles        cycle    453434976
    -------------------------- ----------- ------------

  void vectorized_elementwise_kernel<4, unnamed>::silu_kernel(TensorIteratorBase &)::[lambda() (instance 1)]::operator ()() lambda() (instance 2)]::operator ()() lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3) (524288, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         2.62
    SM Frequency                    Ghz         1.48
    Elapsed Cycles                cycle       865409
    Memory Throughput                 %        90.61
    DRAM Throughput                   %        90.61
    Duration                         us       582.40
    L1/TEX Cache Throughput           %        33.40
    L2 Cache Throughput               %        82.84
    SM Active Cycles              cycle    856262.14
    Compute (SM) Throughput           %        47.62
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 524288
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Stack Size                                                  1024
    Threads                                   thread        67108864
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                              248.24
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block           21
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        83.97
    Achieved Active Warps Per SM           warp        53.74
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 16.03%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (84.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   1381986.50
    Total DRAM Elapsed Cycles        cycle     73207808
    Average L1 Active Cycles         cycle    856262.14
    Total L1 Elapsed Cycles          cycle    113424438
    Average L2 Active Cycles         cycle    984616.67
    Total L2 Elapsed Cycles          cycle     95215584
    Average SM Active Cycles         cycle    856262.14
    Total SM Elapsed Cycles          cycle    113424438
    Average SMSP Active Cycles       cycle    857516.51
    Total SMSP Elapsed Cycles        cycle    453697752
    -------------------------- ----------- ------------

  void vectorized_elementwise_kernel<4, unnamed>::silu_kernel(TensorIteratorBase &)::[lambda() (instance 1)]::operator ()() lambda() (instance 2)]::operator ()() lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3) (524288, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         2.62
    SM Frequency                    Ghz         1.48
    Elapsed Cycles                cycle       867423
    Memory Throughput                 %        90.89
    DRAM Throughput                   %        90.89
    Duration                         us       580.64
    L1/TEX Cache Throughput           %        33.75
    L2 Cache Throughput               %        83.27
    SM Active Cycles              cycle    847340.22
    Compute (SM) Throughput           %        47.61
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 524288
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Stack Size                                                  1024
    Threads                                   thread        67108864
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                              248.24
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block           21
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        83.96
    Achieved Active Warps Per SM           warp        53.73
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 16.04%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (84.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   1382161.17
    Total DRAM Elapsed Cycles        cycle     72990720
    Average L1 Active Cycles         cycle    847340.22
    Total L1 Elapsed Cycles          cycle    113448702
    Average L2 Active Cycles         cycle    986009.69
    Total L2 Elapsed Cycles          cycle     94931808
    Average SM Active Cycles         cycle    847340.22
    Total SM Elapsed Cycles          cycle    113448702
    Average SMSP Active Cycles       cycle    854917.22
    Total SMSP Elapsed Cycles        cycle    453794808
    -------------------------- ----------- ------------

  void vectorized_elementwise_kernel<4, unnamed>::silu_kernel(TensorIteratorBase &)::[lambda() (instance 1)]::operator ()() lambda() (instance 2)]::operator ()() lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3) (524288, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         2.62
    SM Frequency                    Ghz         1.48
    Elapsed Cycles                cycle       871169
    Memory Throughput                 %        90.70
    DRAM Throughput                   %        90.70
    Duration                         us       581.79
    L1/TEX Cache Throughput           %        33.83
    L2 Cache Throughput               %        82.95
    SM Active Cycles              cycle    845212.05
    Compute (SM) Throughput           %        47.39
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 524288
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Stack Size                                                  1024
    Threads                                   thread        67108864
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                              248.24
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block           21
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        83.94
    Achieved Active Warps Per SM           warp        53.72
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 16.06%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (83.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   1381951.50
    Total DRAM Elapsed Cycles        cycle     73134208
    Average L1 Active Cycles         cycle    845212.05
    Total L1 Elapsed Cycles          cycle    113977710
    Average L2 Active Cycles         cycle    986773.52
    Total L2 Elapsed Cycles          cycle     95092416
    Average SM Active Cycles         cycle    845212.05
    Total SM Elapsed Cycles          cycle    113977710
    Average SMSP Active Cycles       cycle    841976.53
    Total SMSP Elapsed Cycles        cycle    455910840
    -------------------------- ----------- ------------

  void vectorized_elementwise_kernel<4, unnamed>::silu_kernel(TensorIteratorBase &)::[lambda() (instance 1)]::operator ()() lambda() (instance 2)]::operator ()() lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3) (524288, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         2.62
    SM Frequency                    Ghz         1.48
    Elapsed Cycles                cycle       867525
    Memory Throughput                 %        90.71
    DRAM Throughput                   %        90.71
    Duration                         us       581.82
    L1/TEX Cache Throughput           %        33.35
    L2 Cache Throughput               %        82.97
    SM Active Cycles              cycle    857611.77
    Compute (SM) Throughput           %        47.58
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 524288
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Stack Size                                                  1024
    Threads                                   thread        67108864
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                              248.24
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block           21
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        83.98
    Achieved Active Warps Per SM           warp        53.75
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 16.02%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (84.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   1382113.17
    Total DRAM Elapsed Cycles        cycle     73136640
    Average L1 Active Cycles         cycle    857611.77
    Total L1 Elapsed Cycles          cycle    113504320
    Average L2 Active Cycles         cycle    984231.88
    Total L2 Elapsed Cycles          cycle     95107296
    Average SM Active Cycles         cycle    857611.77
    Total SM Elapsed Cycles          cycle    113504320
    Average SMSP Active Cycles       cycle    853523.00
    Total SMSP Elapsed Cycles        cycle    454017280
    -------------------------- ----------- ------------

  void vectorized_elementwise_kernel<4, unnamed>::silu_kernel(TensorIteratorBase &)::[lambda() (instance 1)]::operator ()() lambda() (instance 2)]::operator ()() lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3) (524288, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         2.62
    SM Frequency                    Ghz         1.47
    Elapsed Cycles                cycle       863497
    Memory Throughput                 %        90.78
    DRAM Throughput                   %        90.78
    Duration                         us       581.28
    L1/TEX Cache Throughput           %        33.79
    L2 Cache Throughput               %        83.03
    SM Active Cycles              cycle    846270.56
    Compute (SM) Throughput           %        47.91
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 524288
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Stack Size                                                  1024
    Threads                                   thread        67108864
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                              248.24
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block           21
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        83.98
    Achieved Active Warps Per SM           warp        53.75
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 16.02%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (84.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   1381937.17
    Total DRAM Elapsed Cycles        cycle     73070080
    Average L1 Active Cycles         cycle    846270.56
    Total L1 Elapsed Cycles          cycle    112729610
    Average L2 Active Cycles         cycle    984557.31
    Total L2 Elapsed Cycles          cycle     95010720
    Average SM Active Cycles         cycle    846270.56
    Total SM Elapsed Cycles          cycle    112729610
    Average SMSP Active Cycles       cycle    852205.93
    Total SMSP Elapsed Cycles        cycle    450918440
    -------------------------- ----------- ------------

  void vectorized_elementwise_kernel<4, unnamed>::silu_kernel(TensorIteratorBase &)::[lambda() (instance 1)]::operator ()() lambda() (instance 2)]::operator ()() lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3) (524288, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         2.62
    SM Frequency                    Ghz         1.48
    Elapsed Cycles                cycle       865381
    Memory Throughput                 %        90.73
    DRAM Throughput                   %        90.73
    Duration                         us       581.63
    L1/TEX Cache Throughput           %        33.62
    L2 Cache Throughput               %        83.07
    SM Active Cycles              cycle    850593.72
    Compute (SM) Throughput           %        47.63
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 524288
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Stack Size                                                  1024
    Threads                                   thread        67108864
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                              248.24
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block           21
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        83.89
    Achieved Active Warps Per SM           warp        53.69
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 16.11%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (83.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   1382064.33
    Total DRAM Elapsed Cycles        cycle     73115136
    Average L1 Active Cycles         cycle    850593.72
    Total L1 Elapsed Cycles          cycle    113401554
    Average L2 Active Cycles         cycle    986991.12
    Total L2 Elapsed Cycles          cycle     95105376
    Average SM Active Cycles         cycle    850593.72
    Total SM Elapsed Cycles          cycle    113401554
    Average SMSP Active Cycles       cycle    842558.73
    Total SMSP Elapsed Cycles        cycle    453606216
    -------------------------- ----------- ------------

  void vectorized_elementwise_kernel<4, unnamed>::silu_kernel(TensorIteratorBase &)::[lambda() (instance 1)]::operator ()() lambda() (instance 2)]::operator ()() lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3) (524288, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         2.62
    SM Frequency                    Ghz         1.47
    Elapsed Cycles                cycle       864628
    Memory Throughput                 %        90.75
    DRAM Throughput                   %        90.75
    Duration                         us       581.47
    L1/TEX Cache Throughput           %        33.20
    L2 Cache Throughput               %        82.90
    SM Active Cycles              cycle    861264.84
    Compute (SM) Throughput           %        47.75
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 524288
    Registers Per Thread             register/thread              23
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Stack Size                                                  1024
    Threads                                   thread        67108864
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                              248.24
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block           21
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        83.97
    Achieved Active Warps Per SM           warp        53.74
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 16.03%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (84.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   1382009.33
    Total DRAM Elapsed Cycles        cycle     73095168
    Average L1 Active Cycles         cycle    861264.84
    Total L1 Elapsed Cycles          cycle    113099834
    Average L2 Active Cycles         cycle    986742.84
    Total L2 Elapsed Cycles          cycle     95052096
    Average SM Active Cycles         cycle    861264.84
    Total SM Elapsed Cycles          cycle    113099834
    Average SMSP Active Cycles       cycle    856125.88
    Total SMSP Elapsed Cycles        cycle    452399336
    -------------------------- ----------- ------------

